{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score, make_index_balanced_accuracy, specificity_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import average\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "SEED = 42\n",
    "DATA_STRUCTURE = json.load(open('data_structure.json'))\n",
    "DATA_KEYS = list(DATA_STRUCTURE.keys())\n",
    "CLASSIFIERS = {\n",
    "  \"DecisionTreeClassifier\":DecisionTreeClassifier,\n",
    "}\n",
    "OUTPUT_COLS = ['dataset', 'target', 'target_name', 'avg_precision', 'precision_folds', 'avg_recall', 'recall_folds', 'avg_f1', 'f1_folds', 'avg_geometric_mean', 'geometric_mean_folds','avg_specificity','specificity_folds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "  if name == 'imdb':\n",
    "    movie_data = pd.read_csv('./Datasets/movie_metadata.csv')\n",
    "    movie_data.drop_duplicates(inplace=True)\n",
    "    movie_data.loc[:,'genres'] = movie_data.loc[:,'genres'].apply(lambda x: x.split('|'))\n",
    "\n",
    "    genres = []\n",
    "    for x in movie_data.genres:\n",
    "      for g in x:\n",
    "        if g not in genres:\n",
    "          genres.append(g)\n",
    "\n",
    "    for g in genres:\n",
    "      movie_data.loc[:,g] = movie_data.loc[:,'genres'].apply(lambda x: int(g in x))\n",
    "\n",
    "    movie_data.drop([\"director_name\",\"actor_2_name\",\"movie_title\",\"genres\",\"actor_1_name\",\"actor_3_name\",\"plot_keywords\",\"movie_imdb_link\",\"cast_total_facebook_likes\"],axis=1, inplace = True)\n",
    "    movie_data.dropna(inplace=True)\n",
    "\n",
    "    movie_data[\"imdb_score\"] = movie_data[\"imdb_score\"].apply(float)\n",
    "    movie_data.loc[movie_data['imdb_score'].between(8,10), 'imdb_score'] = 100.0\n",
    "    movie_data.loc[movie_data['imdb_score'].between(5,7.99), 'imdb_score'] = 50.0\n",
    "    movie_data.loc[movie_data['imdb_score'].between(0,4.992), 'imdb_score'] = 30.0\n",
    "    movie_data[\"imdb_score\"] = movie_data[\"imdb_score\"].apply(str)\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"100.0\", 'imdb_score'] = \"GOOD\"\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"50.0\", 'imdb_score'] = \"AVERAGE\"\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"30.0\", 'imdb_score'] = \"BAD\"\n",
    "\n",
    "    ratings = movie_data[\"content_rating\"].unique()\n",
    "    for rate in ratings:\n",
    "        if rate == \"M\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        elif rate == \"GP\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        elif rate == \"Unrated\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"Not Rated\"\n",
    "        elif rate == \"Passed\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"Approved\"\n",
    "        elif rate == \"X\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"NC-17\"\n",
    "\n",
    "    movie_data.loc[movie_data['gross'].between(0,15000000.0), 'gross'] = 0.0\n",
    "    movie_data.loc[movie_data['gross'].between(1500000.01,762000000.0), 'gross'] = 1.0\n",
    "    movie_data = pd.get_dummies(movie_data,columns=['color','language','country'],drop_first=True)\n",
    "\n",
    "    target1 = le.fit_transform(movie_data[\"imdb_score\"])\n",
    "    target2 = le.fit_transform(movie_data[\"content_rating\"])\n",
    "    target3 = le.fit_transform(movie_data[\"gross\"])\n",
    "    \n",
    "    X_final = movie_data.drop([\"imdb_score\", \"content_rating\", \"gross\"],axis=1)\n",
    "\n",
    "  elif name == 'mushroom':\n",
    "    mushroom_data = pd.read_csv('./Datasets/mushroom.csv')\n",
    "    mushroom_data = mushroom_data[mushroom_data['stalk-root']!='?']\n",
    "    mushroom_data = pd.get_dummies(mushroom_data,columns=['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "          'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "          'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "          'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "          'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
    "          'ring-type', 'spore-print-color'],drop_first=True)\n",
    "    target1 = le.fit_transform(mushroom_data[\"Class\"])\n",
    "    target2 = le.fit_transform(mushroom_data[\"population\"])\n",
    "    target3 = le.fit_transform(mushroom_data[\"habitat\"])\n",
    "    X_final = mushroom_data.drop([\"Class\", \"population\", \"habitat\"],axis=1)\n",
    "  \n",
    "  elif name == 'census':\n",
    "    census_data = pd.read_csv('./Datasets/census.csv',names=['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']) \n",
    "    census_data=census_data.drop_duplicates()\n",
    "    for col in census_data.columns:\n",
    "      census_data = census_data[census_data[col]!=' ?']\n",
    "    census_data.loc[census_data.income==' <=50K.','income'] = ' <=50K' \n",
    "    census_data.loc[census_data.income==' >50K.','income'] = ' >50K'\n",
    "    census_data = pd.get_dummies(census_data,columns=['education','occupation','relationship','race','sex','native-country'],drop_first=True)\n",
    "    target1 = le.fit_transform(census_data[\"income\"])\n",
    "    target2 = le.fit_transform(census_data[\"marital-status\"])\n",
    "    target3 = le.fit_transform(census_data[\"workclass\"])\n",
    "    X_final = census_data.drop([\"income\", \"marital-status\", \"workclass\"],axis=1)\n",
    "  \n",
    "  elif name == 'bank':\n",
    "    bank_data = pd.read_csv('./Datasets/bank-additional.csv')\n",
    "    for col in bank_data.columns:\n",
    "      bank_data = bank_data[bank_data[col]!='unknown']\n",
    "    bank_data = pd.get_dummies(bank_data,columns=['job','education','default','contact','month','day_of_week','poutcome','marital'],drop_first=True)\n",
    "    target1 = le.fit_transform(bank_data[\"y\"])\n",
    "    target2 = le.fit_transform(bank_data[\"loan\"])\n",
    "    target3 = le.fit_transform(bank_data[\"housing\"])\n",
    "    X_final = bank_data.drop([\"y\", \"loan\", \"housing\"],axis=1)\n",
    "\n",
    "  elif name == 'intention':\n",
    "    intention_data = pd.read_csv('./Datasets/online_shoppers_intention.csv')\n",
    "    intention_data = intention_data.drop_duplicates()\n",
    "    intention_data = intention_data[intention_data['VisitorType']!='Other']\n",
    "    intention_data = pd.get_dummies(intention_data,columns=['Month','Weekend'],drop_first=True)        \n",
    "    target1 = le.fit_transform(intention_data[\"Revenue\"])\n",
    "    target2 = le.fit_transform(intention_data[\"VisitorType\"])\n",
    "    target3 = le.fit_transform(intention_data[\"SpecialDay\"])\n",
    "    X_final = intention_data.drop([\"Revenue\", \"VisitorType\", \"SpecialDay\"],axis=1)\n",
    "\n",
    "  elif name == 'anuran':\n",
    "    anuran_data = pd.read_csv('./Datasets/Frogs_MFCCs.csv')\n",
    "    anuran_data.drop(columns='RecordID',inplace=True)\n",
    "    target1 = le.fit_transform(anuran_data[\"Family\"])\n",
    "    target2 = le.fit_transform(anuran_data[\"Genus\"])\n",
    "    target3 = le.fit_transform(anuran_data[\"Species\"])\n",
    "    X_final = anuran_data.drop([\"Family\", \"Genus\", \"Species\"],axis=1)\n",
    "  \n",
    "  elif name == 'telco':\n",
    "    telco_data = pd.read_csv('./Datasets/telco.csv')\n",
    "    telco_data.drop(columns=['customerID'],inplace=True)\n",
    "    telco_data = telco_data.drop_duplicates()\n",
    "    telco_data = telco_data[telco_data.TotalCharges!=' ']\n",
    "    telco_data = pd.get_dummies(telco_data,columns=['gender','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling'],drop_first=True)\n",
    "    target1 = le.fit_transform(telco_data[\"Churn\"])\n",
    "    target2 = le.fit_transform(telco_data[\"Contract\"])\n",
    "    target3 = le.fit_transform(telco_data[\"PaymentMethod\"])\n",
    "    X_final = telco_data.drop([\"Churn\", \"Contract\", \"PaymentMethod\"],axis=1)\n",
    "\n",
    "  elif name == 'paris':\n",
    "    paris_data = pd.read_csv('./Datasets/ParisHousingClass.csv')\n",
    "    paris_data.drop(columns='made',inplace=True)\n",
    "    target1 = le.fit_transform(paris_data[\"category\"])\n",
    "    target2 = le.fit_transform(paris_data[\"isNewBuilt\"])\n",
    "    target3 = le.fit_transform(paris_data[\"hasStorageRoom\"])\n",
    "    X_final = paris_data.drop([\"category\", \"isNewBuilt\", \"hasStorageRoom\"],axis=1)\n",
    "  \n",
    "  elif name == 'smoking':\n",
    "    smoking_data = pd.read_csv('./Datasets/smoking.csv')\n",
    "    smoking_data.drop(columns=['ID','oral'],inplace=True)\n",
    "    smoking_data = smoking_data.drop_duplicates()\n",
    "    smoking_data = pd.get_dummies(smoking_data,columns=['gender'],drop_first=True)\n",
    "    target1 = le.fit_transform(smoking_data[\"smoking\"])\n",
    "    target2 = le.fit_transform(smoking_data[\"tartar\"])\n",
    "    target3 = le.fit_transform(smoking_data[\"dental caries\"])\n",
    "    X_final = smoking_data.drop([\"smoking\", \"tartar\", \"dental caries\"],axis=1)\n",
    "  \n",
    "  elif name == 'flight':\n",
    "    flight_data = pd.read_csv('./Datasets/flight.csv')\n",
    "    flight_data.dropna(inplace=True)\n",
    "    flight_data = pd.get_dummies(flight_data,columns=['Gender','Type of Travel'],drop_first=True)\n",
    "    target1 = le.fit_transform(flight_data[\"satisfaction\"])\n",
    "    target2 = le.fit_transform(flight_data[\"Customer Type\"])\n",
    "    target3 = le.fit_transform(flight_data[\"Class\"])\n",
    "    X_final = flight_data.drop([\"satisfaction\", \"Customer Type\", \"Class\"],axis=1)\n",
    "  \n",
    "  else:\n",
    "    raise ValueError('Incorrect dataset')\n",
    "  \n",
    "  return X_final, target1, target2, target3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec_macro 0.7044932628716871\n",
      "recall_macro 0.703390884122784\n",
      "f1_macro 0.7006479368940736\n",
      "geometric_mean 0.6547973751059588\n",
      "specificity 0.7639526174154306\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "def split(df, t, k):\n",
    "  chunks = np.array_split(df, k)\n",
    "  classes = np.array_split(t, k)\n",
    "  return chunks, classes\n",
    "\n",
    "metrics = {\n",
    "  'prec_macro': [],\n",
    "  'recall_macro': [],\n",
    "  'f1_macro': [],\n",
    "  'geometric_mean': [],\n",
    "  'specificity': []\n",
    "}\n",
    "\n",
    "for name in CLASSIFIERS:\n",
    "  # print(name+ ': Starting')\n",
    "  outputDf = pd.DataFrame()\n",
    "  for i in range(len(DATA_STRUCTURE)):\n",
    "    # print('   '+ DATA_KEYS[i] + ': starting')\n",
    "    df, target1, target2, target3 = get_data(DATA_KEYS[i])\n",
    "    data, classes = split(df, target1, 5)\n",
    "\n",
    "    for i in range(5):\n",
    "      testData = data[i]\n",
    "      testClasses = classes[i]\n",
    "      \n",
    "      # use the data at all positions other than i as the train data\n",
    "      trainData = pd.concat((data[:i] + data[i+1:]))\n",
    "      trainclasses = np.concatenate((classes[:i] + classes[i+1:]))\n",
    "      clf = DecisionTreeClassifier()\n",
    "      clf.fit(trainData, trainclasses)\n",
    "      predicted = clf.predict(X=testData)\n",
    "\n",
    "      prec_macro = precision_score(testClasses, predicted, average='macro', zero_division=0)\n",
    "      recall_macro = recall_score(testClasses, predicted, average=\"macro\", zero_division=0)\n",
    "      f1_macro = f1_score(testClasses, predicted, average='macro', zero_division=0)\n",
    "      geometric_mean = geometric_mean_score(testClasses, predicted)\n",
    "      specificity = specificity_score(testClasses, predicted, average='macro')\n",
    "\n",
    "      metrics['prec_macro'].append(prec_macro)\n",
    "      metrics['recall_macro'].append(recall_macro)\n",
    "      metrics['f1_macro'].append(f1_macro)\n",
    "      metrics['geometric_mean'].append(geometric_mean)\n",
    "      metrics['specificity'].append(specificity)\n",
    "\n",
    "for metric in metrics:\n",
    "  print(metric, sum(metrics[metric])/len(metrics[metric]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier: Starting\n",
      "   imdb: starting\n",
      "   mushroom: starting\n",
      "   census: starting\n",
      "   bank: starting\n",
      "   intention: starting\n",
      "   anuran: starting\n",
      "   telco: starting\n",
      "   paris: starting\n",
      "   smoking: starting\n",
      "   flight: starting\n",
      "prec_macro 0.70582543180367\n",
      "recall_macro 0.7007379061438858\n",
      "f1_macro 0.699920898935508\n",
      "geometric_mean 0.6521563098940139\n",
      "specificity 0.7616400462748742\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE\n",
    "\n",
    "\n",
    "def split(df, t, k):\n",
    "  chunks = np.array_split(df, k)\n",
    "  classes = np.array_split(t, k)\n",
    "  return chunks, classes\n",
    "\n",
    "metrics = {\n",
    "  'prec_macro': [],\n",
    "  'recall_macro': [],\n",
    "  'f1_macro': [],\n",
    "  'geometric_mean': [],\n",
    "  'specificity': []\n",
    "}\n",
    "\n",
    "for name in CLASSIFIERS:\n",
    "  print(name+ ': Starting')\n",
    "  outputDf = pd.DataFrame()\n",
    "  for i in range(len(DATA_STRUCTURE)):\n",
    "    print('   '+ DATA_KEYS[i] + ': starting')\n",
    "    df, target1, target2, target3 = get_data(DATA_KEYS[i])\n",
    "    data, classes = split(df, target1, 5)\n",
    "\n",
    "    for i in range(5):\n",
    "      testData = data[i]\n",
    "      testClasses = classes[i]\n",
    "      \n",
    "      # use the data at all positions other than i as the train data\n",
    "      trainData = pd.concat((data[:i] + data[i+1:]))\n",
    "      trainclasses = np.concatenate((classes[:i] + classes[i+1:]))\n",
    "\n",
    "      sm = SMOTE(random_state=SEED, sampling_strategy='not majority')\n",
    "\n",
    "      reSampled_df, resampled_target1 = sm.fit_resample(trainData, trainclasses)\n",
    "\n",
    "\n",
    "      clf = DecisionTreeClassifier()\n",
    "      clf.fit(trainData, trainclasses)\n",
    "      predicted = clf.predict(X=testData)\n",
    "\n",
    "      prec_macro = precision_score(testClasses, predicted, average='macro', zero_division=0)\n",
    "      recall_macro = recall_score(testClasses, predicted, average=\"macro\", zero_division=0)\n",
    "      f1_macro = f1_score(testClasses, predicted, average='macro', zero_division=0)\n",
    "      geometric_mean = geometric_mean_score(testClasses, predicted)\n",
    "      specificity = specificity_score(testClasses, predicted, average='macro')\n",
    "\n",
    "      metrics['prec_macro'].append(prec_macro)\n",
    "      metrics['recall_macro'].append(recall_macro)\n",
    "      metrics['f1_macro'].append(f1_macro)\n",
    "      metrics['geometric_mean'].append(geometric_mean)\n",
    "      metrics['specificity'].append(specificity)\n",
    "\n",
    "for metric in metrics:\n",
    "  print(metric, sum(metrics[metric])/len(metrics[metric]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('csi4900')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c5f941b3679d86a3337c94acfde891d9c2e84f639f121f0c3784daecf41f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
