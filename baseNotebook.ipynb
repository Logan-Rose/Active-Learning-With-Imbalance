{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "  if name == 'imdb':\n",
    "    movie_data = pd.read_csv('./Datasets/movie_metadata.csv')\n",
    "    movie_data.drop_duplicates(inplace=True)\n",
    "    movie_data.loc[:,'genres'] = movie_data.loc[:,'genres'].apply(lambda x: x.split('|'))\n",
    "\n",
    "    genres = []\n",
    "    for x in movie_data.genres:\n",
    "      for g in x:\n",
    "        if g not in genres:\n",
    "          genres.append(g)\n",
    "\n",
    "    for g in genres:\n",
    "      movie_data.loc[:,g] = movie_data.loc[:,'genres'].apply(lambda x: int(g in x))\n",
    "\n",
    "    movie_data.drop([\"director_name\",\"actor_2_name\",\"movie_title\",\"genres\",\"actor_1_name\",\"actor_3_name\",\"plot_keywords\",\"movie_imdb_link\",\"cast_total_facebook_likes\"],axis=1, inplace = True)\n",
    "    movie_data.dropna(inplace=True)\n",
    "\n",
    "    movie_data[\"imdb_score\"] = movie_data[\"imdb_score\"].apply(float)\n",
    "    movie_data.loc[movie_data['imdb_score'].between(8,10), 'imdb_score'] = 100.0\n",
    "    movie_data.loc[movie_data['imdb_score'].between(5,7.99), 'imdb_score'] = 50.0\n",
    "    movie_data.loc[movie_data['imdb_score'].between(0,4.992), 'imdb_score'] = 30.0\n",
    "    movie_data[\"imdb_score\"] = movie_data[\"imdb_score\"].apply(str)\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"100.0\", 'imdb_score'] = \"GOOD\"\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"50.0\", 'imdb_score'] = \"AVERAGE\"\n",
    "    movie_data.loc[movie_data['imdb_score'] == \"30.0\", 'imdb_score'] = \"BAD\"\n",
    "\n",
    "    ratings = movie_data[\"content_rating\"].unique()\n",
    "    for rate in ratings:\n",
    "        if rate == \"M\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        elif rate == \"GP\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        elif rate == \"Unrated\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"Not Rated\"\n",
    "        elif rate == \"Passed\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"Approved\"\n",
    "        elif rate == \"X\":\n",
    "            movie_data.loc[movie_data['content_rating'] == rate, 'content_rating'] = \"NC-17\"\n",
    "\n",
    "    movie_data.loc[movie_data['gross'].between(0,15000000.0), 'gross'] = 0.0\n",
    "    movie_data.loc[movie_data['gross'].between(1500000.01,762000000.0), 'gross'] = 1.0\n",
    "    movie_data = pd.get_dummies(movie_data,columns=['color','language','country'],drop_first=True)\n",
    "\n",
    "    target1 = le.fit_transform(movie_data[\"imdb_score\"])\n",
    "    target2 = le.fit_transform(movie_data[\"content_rating\"])\n",
    "    target3 = le.fit_transform(movie_data[\"gross\"])\n",
    "    \n",
    "    X_final = movie_data.drop([\"imdb_score\", \"content_rating\", \"content_rating\"],axis=1)\n",
    "\n",
    "  elif name == 'mushroom':\n",
    "    mushroom_data = pd.read_csv('./Datasets/mushroom.csv')\n",
    "    mushroom_data = mushroom_data[mushroom_data['stalk-root']!='?']\n",
    "    mushroom_data = pd.get_dummies(mushroom_data,columns=['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "          'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "          'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "          'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "          'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
    "          'ring-type', 'spore-print-color'],drop_first=True)\n",
    "    target1 = le.fit_transform(mushroom_data[\"Class\"])\n",
    "    target2 = le.fit_transform(mushroom_data[\"population\"])\n",
    "    target3 = le.fit_transform(mushroom_data[\"habitat\"])\n",
    "    X_final = mushroom_data.drop([\"Class\", \"population\", \"habitat\"],axis=1)\n",
    "  \n",
    "  elif name == 'census':\n",
    "    census_data = pd.read_csv('./Datasets/census.csv',names=['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']) \n",
    "    census_data=census_data.drop_duplicates()\n",
    "    for col in census_data.columns:\n",
    "      census_data = census_data[census_data[col]!=' ?']\n",
    "    census_data.loc[census_data.income==' <=50K.','income'] = ' <=50K' \n",
    "    census_data.loc[census_data.income==' >50K.','income'] = ' >50K'\n",
    "    census_data = pd.get_dummies(census_data,columns=['education','occupation','relationship','race','sex','native-country'],drop_first=True)\n",
    "    target1 = le.fit_transform(census_data[\"income\"])\n",
    "    target2 = le.fit_transform(census_data[\"marital-status\"])\n",
    "    target3 = le.fit_transform(census_data[\"workclass\"])\n",
    "    X_final = census_data.drop([\"income\", \"marital-status\", \"workclass\"],axis=1)\n",
    "  \n",
    "  elif name == 'bank':\n",
    "    bank_data = pd.read_csv('./Datasets/bank-additional.csv')\n",
    "    for col in bank_data.columns:\n",
    "      bank_data = bank_data[bank_data[col]!='unknown']\n",
    "    bank_data = pd.get_dummies(bank_data,columns=['job','education','default','contact','month','day_of_week','poutcome','marital'],drop_first=True)\n",
    "    target1 = le.fit_transform(bank_data[\"y\"])\n",
    "    target2 = le.fit_transform(bank_data[\"loan\"])\n",
    "    target3 = le.fit_transform(bank_data[\"housing\"])\n",
    "    X_final = bank_data.drop([\"y\", \"loan\", \"housing\"],axis=1)\n",
    "\n",
    "  elif name == 'intention':\n",
    "    intention_data = pd.read_csv('./Datasets/online_shoppers_intention.csv')\n",
    "    intention_data = intention_data.drop_duplicates()\n",
    "    intention_data = intention_data[intention_data['VisitorType']!='Other']\n",
    "    intention_data = pd.get_dummies(intention_data,columns=['Month','Weekend'],drop_first=True)        \n",
    "    target1 = le.fit_transform(intention_data[\"Revenue\"])\n",
    "    target2 = le.fit_transform(intention_data[\"VisitorType\"])\n",
    "    target3 = le.fit_transform(intention_data[\"SpecialDay\"])\n",
    "    X_final = intention_data.drop([\"Revenue\", \"VisitorType\", \"SpecialDay\"],axis=1)\n",
    "\n",
    "  elif name == 'anuran':\n",
    "    anuran_data = pd.read_csv('./Datasets/Frogs_MFCCs.csv')\n",
    "    anuran_data.drop(columns='RecordID',inplace=True)\n",
    "    target1 = le.fit_transform(anuran_data[\"Family\"])\n",
    "    target2 = le.fit_transform(anuran_data[\"Genus\"])\n",
    "    target3 = le.fit_transform(anuran_data[\"Species\"])\n",
    "    X_final = anuran_data.drop([\"Family\", \"Genus\", \"Species\"],axis=1)\n",
    "  \n",
    "  elif name == 'telco':\n",
    "    telco_data = pd.read_csv('./Datasets/telco.csv')\n",
    "    telco_data.drop(columns=['customerID'],inplace=True)\n",
    "    telco_data = telco_data.drop_duplicates()\n",
    "    telco_data = telco_data[telco_data.TotalCharges!=' ']\n",
    "    telco_data = pd.get_dummies(telco_data,columns=['gender','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling'],drop_first=True)\n",
    "    target1 = le.fit_transform(telco_data[\"Churn\"])\n",
    "    target2 = le.fit_transform(telco_data[\"Contract\"])\n",
    "    target3 = le.fit_transform(telco_data[\"PaymentMethod\"])\n",
    "    X_final = telco_data.drop([\"Churn\", \"Contract\", \"PaymentMethod\"],axis=1)\n",
    "\n",
    "  elif name == 'paris':\n",
    "    paris_data = pd.read_csv('./Datasets/ParisHousingClass.csv')\n",
    "    paris_data.drop(columns='made',inplace=True)\n",
    "    target1 = le.fit_transform(paris_data[\"category\"])\n",
    "    target2 = le.fit_transform(paris_data[\"isNewBuilt\"])\n",
    "    target3 = le.fit_transform(paris_data[\"hasStorageRoom\"])\n",
    "    X_final = paris_data.drop([\"category\", \"isNewBuilt\", \"hasStorageRoom\"],axis=1)\n",
    "  \n",
    "  elif name == 'smoking':\n",
    "    smoking_data = pd.read_csv('./Datasets/smoking.csv')\n",
    "    smoking_data.drop(columns=['ID','oral'],inplace=True)\n",
    "    smoking_data = smoking_data.drop_duplicates()\n",
    "    smoking_data = pd.get_dummies(smoking_data,columns=['gender'],drop_first=True)\n",
    "    target1 = le.fit_transform(smoking_data[\"smoking\"])\n",
    "    target2 = le.fit_transform(smoking_data[\"tartar\"])\n",
    "    target3 = le.fit_transform(smoking_data[\"dental caries\"])\n",
    "    X_final = smoking_data.drop([\"smoking\", \"tartar\", \"dental caries\"],axis=1)\n",
    "  \n",
    "  elif name == 'flight':\n",
    "    flight_data = pd.read_csv('./Datasets/flight.csv')\n",
    "    flight_data.dropna(inplace=True)\n",
    "    flight_data = pd.get_dummies(flight_data,columns=['Gender','Type of Travel'],drop_first=True)\n",
    "    target1 = le.fit_transform(flight_data[\"satisfaction\"])\n",
    "    target2 = le.fit_transform(flight_data[\"Customer Type\"])\n",
    "    target3 = le.fit_transform(flight_data[\"Class\"])\n",
    "    X_final = flight_data.drop([\"satisfaction\", \"Customer Type\", \"Class\"],axis=1)\n",
    "  \n",
    "  else:\n",
    "    raise ValueError('Incorrect dataset')\n",
    "  \n",
    "  return X_final, target1, target2, target3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDist(arr, title):\n",
    "  values, counts = np.unique(arr, return_counts=True)\n",
    "  lis = np.asarray((values, counts)).T\n",
    "  x, y = zip(*lis) # unpack a list of pairs into two tuples\n",
    "  # plt.plot(x, y)\n",
    "  plt.bar(x,y)\n",
    "\n",
    "  plt.title(title)\n",
    "  # plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 50), dpi=80)\n",
    "\n",
    "data = json.load(open('data_structure.json'))\n",
    "dataKeys = list(data.keys())\n",
    "for i in range(len(data)):\n",
    "  key = dataKeys[i]\n",
    "  dataSet = data[key]\n",
    "  target, t1, t2, t3 = get_data(key)\n",
    "  \n",
    "  plt.subplot(10, 3, i*3+1)\n",
    "  plotDist(t1, dataSet['t1'])\n",
    "  plt.subplot(10, 3, i*3+2)\n",
    "  plotDist(t2, dataSet['t2'])\n",
    "  plt.subplot(10,3, i*3+3)\n",
    "  plotDist(t3, dataSet['t3'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------[ Classification Report for imdb ]--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93       646\n",
      "           1       0.00      0.00      0.00        69\n",
      "           2       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.86       748\n",
      "   macro avg       0.29      0.33      0.31       748\n",
      "weighted avg       0.75      0.86      0.80       748\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/loganrose/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.00      0.00      0.00       130\n",
      "           5       0.44      0.27      0.33       257\n",
      "           6       0.49      0.88      0.63       328\n",
      "\n",
      "    accuracy                           0.48       748\n",
      "   macro avg       0.13      0.16      0.14       748\n",
      "weighted avg       0.36      0.48      0.39       748\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.69      0.64       258\n",
      "           1       0.82      0.75      0.78       490\n",
      "\n",
      "    accuracy                           0.73       748\n",
      "   macro avg       0.71      0.72      0.71       748\n",
      "weighted avg       0.74      0.73      0.73       748\n",
      "\n",
      "--------------------------[ Classification Report for mushroom ]--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       687\n",
      "           1       1.00      1.00      1.00       442\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        82\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        60\n",
      "           3       1.00      1.00      1.00       198\n",
      "           4       1.00      1.00      1.00       448\n",
      "           5       1.00      1.00      1.00       330\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       476\n",
      "           1       1.00      1.00      1.00       385\n",
      "           2       1.00      1.00      1.00        12\n",
      "           3       1.00      1.00      1.00        56\n",
      "           4       1.00      1.00      1.00       137\n",
      "           5       1.00      1.00      1.00        63\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n",
      "--------------------------[ Classification Report for census ]--------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m     43\u001b[0m   key \u001b[38;5;241m=\u001b[39m dataKeys[i]\n\u001b[0;32m---> 44\u001b[0m   getClassificationReports(key)\n",
      "Cell \u001b[0;32mIn [69], line 33\u001b[0m, in \u001b[0;36mgetClassificationReports\u001b[0;34m(dataSet)\u001b[0m\n\u001b[1;32m     30\u001b[0m gaussian\u001b[38;5;241m.\u001b[39mfit(train, trainTarget)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print the classification report\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(testTarget, predictions))\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/svm/_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    789\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m    792\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/svm/_base.py:416\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    414\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    415\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi4900/lib/python3.10/site-packages/sklearn/svm/_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    428\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be equal to \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe number of samples at training time\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m             \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_[\u001b[39m0\u001b[39m])\n\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    433\u001b[0m svm_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n\u001b[0;32m--> 435\u001b[0m \u001b[39mreturn\u001b[39;00m libsvm\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    436\u001b[0m     X,\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_,\n\u001b[1;32m    438\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_vectors_,\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_support,\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dual_coef_,\n\u001b[1;32m    441\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_,\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probA,\n\u001b[1;32m    443\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probB,\n\u001b[1;32m    444\u001b[0m     svm_type\u001b[39m=\u001b[39;49msvm_type,\n\u001b[1;32m    445\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[1;32m    446\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[1;32m    447\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[1;32m    448\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[1;32m    449\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[1;32m    450\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def getClassificationReports(dataSet):\n",
    "  print(f'--------------------------[ Classification Report for {dataSet} ]--------------------------')\n",
    "  df, target1, target2, target3 = get_data(dataSet)\n",
    "\n",
    "  df['target1'] = target1\n",
    "  df['target2'] = target2\n",
    "  df['target3'] = target3\n",
    "\n",
    "  train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "  targets = ['target1', 'target2', 'target3']\n",
    "\n",
    "  for t in targets:\n",
    "    # Get the Target from the training data\n",
    "    trainTarget = train[t]\n",
    "    # Remove it from the training data\n",
    "    train.drop([t], axis=1)\n",
    "\n",
    "    # Get the Target from the testing data\n",
    "    testTarget = test[t]\n",
    "    # Remove it from the training data\n",
    "    test.drop([t], axis=1)\n",
    "\n",
    "    # Create and fit a gaussian Model\n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(train, trainTarget)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = gaussian.predict(test)\n",
    "\n",
    "    # print the classification report\n",
    "    print(classification_report(testTarget, predictions))\n",
    "\n",
    "\n",
    "\n",
    "data = json.load(open('data_structure.json'))\n",
    "dataKeys = list(data.keys())\n",
    "for i in range(len(data)):\n",
    "  key = dataKeys[i]\n",
    "  getClassificationReports(key)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('csi4900')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c5f941b3679d86a3337c94acfde891d9c2e84f639f121f0c3784daecf41f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
